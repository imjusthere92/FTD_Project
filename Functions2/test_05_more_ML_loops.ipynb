{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d7930a5-46ee-4bb0-8994-7a9ce4cb661e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import glob\n",
    "\n",
    "from pathlib import Path\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44bb640e-c6e7-433b-8991-a2e0ba1fa3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99ade8d9-0fb7-4c74-bb35-f3f468564e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load pickle for exports and imports of data  \n",
    "import pickle \n",
    "def load_obj(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def save_obj(obj, path ):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08053691-b3d4-4df0-bc76-340d010096e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9992"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path('../Resources/finished_data_dict.pkl')\n",
    "data_dict_import = load_obj(path)\n",
    "path = Path('../Resources/finished_key_list.csv')\n",
    "key_list_import_df = pd.read_csv(path,index_col=0)\n",
    "key_list = []\n",
    "for i in key_list_import_df['Symbol']:\n",
    "    key_list.append(i)\n",
    "len(key_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dadf9f40-ab9a-4a73-b3e9-7dacc71ec1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float = 0 for 2112 symbols\n",
      "All list length: 3695\n",
      "ETF list length: 1872\n",
      "EQT list length: 1823\n"
     ]
    }
   ],
   "source": [
    "ftd_key_list = []\n",
    "etf_key_list = []\n",
    "eqt_key_list = []\n",
    "float_count_is_0 = 0\n",
    "\n",
    "for i in key_list:\n",
    "    index_value = i\n",
    "    float_ftd_pct_ytd = data_dict_import[index_value]['ftd_stats']['float_ftd_pct_ytd']\n",
    "    ## Check if there's float data. Otherwise, calculate using Outstanding share data\n",
    "    ## ETFs have no float data, so they always use Outstanding Shares \n",
    "    if float_ftd_pct_ytd == 0:        \n",
    "        os_ftd_pct_ytd = data_dict_import[index_value]['ftd_stats']['os_ftd_pct_ytd']\n",
    "        if os_ftd_pct_ytd >= 5:\n",
    "            float_count_is_0 += 1\n",
    "            ftd_key_list.append(i)\n",
    "            isEtf = data_dict_import[index_value]['companyProfile']['isEtf']\n",
    "            if isEtf == True:\n",
    "                etf_key_list.append(i)\n",
    "            else:\n",
    "                eqt_key_list.append(i)\n",
    "    else:\n",
    "        if float_ftd_pct_ytd >= 5:\n",
    "            ftd_key_list.append(i)\n",
    "            isEtf = data_dict_import[index_value]['companyProfile']['isEtf']\n",
    "            if isEtf == 'True':\n",
    "                etf_key_list.append(i)\n",
    "            else:\n",
    "                eqt_key_list.append(i)\n",
    "\n",
    "print(f'Float = 0 for '+str(float_count_is_0)+' symbols')\n",
    "print('All list length:',len(ftd_key_list))\n",
    "print('ETF list length:',len(etf_key_list))\n",
    "print('EQT list length:',len(eqt_key_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d50f45a2-74e1-41c8-86cf-54f37d1cde2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_symbol = 'GME'\n",
    "symbol = test_symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc4cd85d-c606-40c1-9905-0e3b156db7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_dict_import[symbol]['dataFrame'].copy()\n",
    "X = df.drop(columns={'close','adjClose'}).values\n",
    "y = df['close'].values\n",
    "\n",
    "# # Create training and testing datasets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)\n",
    "\n",
    "scaler = StandardScaler().fit(X)\n",
    "X = scaler.transform(X) \n",
    "\n",
    "# Create a sequential model\n",
    "nn = Sequential()\n",
    "\n",
    "num_of_inputs = 16\n",
    "num_of_outputs= 1\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(Dense(units=8, input_dim=num_of_inputs, activation=\"relu\"))\n",
    "\n",
    "# Second hidden layer\n",
    "nn.add(Dense(units=8, activation=\"relu\"))\n",
    "\n",
    "# Output layer\n",
    "nn.add(Dense(units=num_of_outputs, activation=\"linear\"))\n",
    "\n",
    "# Compile the model\n",
    "nn.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[\"mse\"])\n",
    "\n",
    "# Fit the model\n",
    "model = nn.fit(X, y, validation_split=0.3, epochs=800, verbose=0)\n",
    "\n",
    "# Save model_2 as JSON\n",
    "nn_json = nn.to_json()\n",
    "\n",
    "file_path = Path('../Model_Data/'+symbol+'_model_2Layers.json')\n",
    "with open(file_path, \"w\") as json_file:\n",
    "    json_file.write(nn_json)\n",
    "\n",
    "# Save weights\n",
    "file_path = Path('../Model_Data/'+symbol+'_model_2Layers_weights.h5')\n",
    "nn.save_weights(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc7203ae-1023-48aa-8a0b-49846864f574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-04</th>\n",
       "      <td>28.309999</td>\n",
       "      <td>28.336760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-05</th>\n",
       "      <td>28.770000</td>\n",
       "      <td>28.667849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-06</th>\n",
       "      <td>28.370001</td>\n",
       "      <td>28.201527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-07</th>\n",
       "      <td>28.450001</td>\n",
       "      <td>28.441654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-08</th>\n",
       "      <td>28.370001</td>\n",
       "      <td>28.320555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-25</th>\n",
       "      <td>173.970000</td>\n",
       "      <td>172.639984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-26</th>\n",
       "      <td>177.840000</td>\n",
       "      <td>178.294830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-27</th>\n",
       "      <td>173.510000</td>\n",
       "      <td>170.671295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-28</th>\n",
       "      <td>182.850000</td>\n",
       "      <td>182.435333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-29</th>\n",
       "      <td>183.510000</td>\n",
       "      <td>181.068680</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1468 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 close   predicted\n",
       "Date                              \n",
       "2016-01-04   28.309999   28.336760\n",
       "2016-01-05   28.770000   28.667849\n",
       "2016-01-06   28.370001   28.201527\n",
       "2016-01-07   28.450001   28.441654\n",
       "2016-01-08   28.370001   28.320555\n",
       "...                ...         ...\n",
       "2021-10-25  173.970000  172.639984\n",
       "2021-10-26  177.840000  178.294830\n",
       "2021-10-27  173.510000  170.671295\n",
       "2021-10-28  182.850000  182.435333\n",
       "2021-10-29  183.510000  181.068680\n",
       "\n",
       "[1468 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load json and create model\n",
    "file_path = Path('../Model_Data/'+symbol+'_model_2Layers.json')\n",
    "with open(file_path, \"r\") as json_file:\n",
    "    model_json = json_file.read()\n",
    "loaded_model = model_from_json(model_json)\n",
    "\n",
    "# load weights into new model\n",
    "file_path = Path('../Model_Data/'+symbol+'_model_2Layers_weights.h5')\n",
    "loaded_model.load_weights(file_path)\n",
    "\n",
    "df2 = df.copy()\n",
    "df2[\"predicted\"] = loaded_model.predict(X)\n",
    "df_pred = df2[['close','predicted']]\n",
    "df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "720a135b-2897-46da-9db9-6e93e00be0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 - 0s - loss: 6.1345 - mse: 6.1345 - 40ms/epoch - 879us/step\n",
      "Loss: 6.1345367431640625, Accuracy: 6.1345367431640625\n"
     ]
    }
   ],
   "source": [
    "model_loss, model_accuracy = nn.evaluate(X, y, verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cecc9056-60f9-4c7d-b6ce-577608e99df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.1345367431640625"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87b688d1-c44c-414b-ae33-05ca09b66e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9992"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(key_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbdc68f1-1a58-42d5-b195-1b27872e7ea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_key_list = key_list[0:10]\n",
    "len(test_key_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed6ed711-fdc4-4036-b446-77218eaeb8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for symbol in test_key_list:\n",
    "    df = data_dict_import[symbol]['dataFrame'].copy()\n",
    "    X = df.drop(columns={'close','adjClose'}).values\n",
    "    y = df['close'].values\n",
    "    scaler = StandardScaler().fit(X)\n",
    "    X = scaler.transform(X) \n",
    "    num_of_inputs = 16\n",
    "    num_of_outputs= 1\n",
    "    ## Run Model 5 times and save best model \n",
    "    for i in range(5):\n",
    "        ## Initialize Model \n",
    "        nn = Sequential()\n",
    "        \n",
    "    # First hidden layer\n",
    "    nn.add(Dense(units=8, input_dim=num_of_inputs, activation=\"relu\"))\n",
    "\n",
    "    # Second hidden layer\n",
    "    nn.add(Dense(units=8, activation=\"relu\"))\n",
    "\n",
    "    # Output layer\n",
    "    nn.add(Dense(units=num_of_outputs, activation=\"linear\"))\n",
    "\n",
    "    # Compile the model\n",
    "    nn.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[\"mse\"])\n",
    "\n",
    "    # Fit the model\n",
    "    model = nn.fit(X, y, validation_split=0.3, epochs=800, verbose=0)\n",
    "\n",
    "#     # Save model as JSON\n",
    "#     nn_json = nn.to_json()\n",
    "\n",
    "#     file_path = Path('../Model_Data_Temp/Temp_model_2Layers.json')\n",
    "#     with open(file_path, \"w\") as json_file:\n",
    "#         json_file.write(nn_json)\n",
    "\n",
    "#     # Save weights\n",
    "#     file_path = Path('../Model_Data_Temp/Temp_model_2Layers_weights.h5')\n",
    "#     nn.save_weights(file_path)\n",
    "    \n",
    "#     # load json and create model\n",
    "#     file_path = Path('../Model_Data/Temp_model_2Layers.json')\n",
    "#     with open(file_path, \"r\") as json_file:\n",
    "#         model_json = json_file.read()\n",
    "#     loaded_model = model_from_json(model_json)\n",
    "\n",
    "#     # load weights into new model\n",
    "#     file_path = Path('../Model_Data/Temp_model_2Layers_weights.h5')\n",
    "#     loaded_model.load_weights(file_path)\n",
    "\n",
    "#     df2 = df.copy()\n",
    "#     df2[\"predicted\"] = loaded_model.predict(X)\n",
    "#     df_pred = df2[['close','predicted']]\n",
    "    \n",
    "    model_loss, model_accuracy = nn.evaluate(X, y, verbose=2)\n",
    "    \n",
    "    if i = 0:\n",
    "        model_accuracy1 = model_accuracy\n",
    "        # Save model as JSON\n",
    "        nn_json = nn.to_json()\n",
    "\n",
    "        file_path = Path('../Model_Data/'+symbol+'_model_2Layers.json')\n",
    "        with open(file_path, \"w\") as json_file:\n",
    "            json_file.write(nn_json)\n",
    "\n",
    "        # Save weights\n",
    "        file_path = Path('../Model_Data/'+symbol+'_model_2Layers_weights.h5')\n",
    "        nn.save_weights(file_path)\n",
    "    else:\n",
    "        if model_accuracy > model_accuracy1:\n",
    "            model_accuracy1 = model_accuracy\n",
    "            # Save model as JSON\n",
    "            nn_json = nn.to_json()\n",
    "\n",
    "            file_path = Path('../Model_Data/'+symbol+'_model_2Layers.json')\n",
    "            with open(file_path, \"w\") as json_file:\n",
    "                json_file.write(nn_json)\n",
    "\n",
    "            # Save weights\n",
    "            file_path = Path('../Model_Data/'+symbol+'_model_2Layers_weights.h5')\n",
    "            nn.save_weights(file_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d08a513-df91-448a-a91f-6494d92452ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
