{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c449c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import glob\n",
    "\n",
    "from pathlib import Path\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94368b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "117cc216",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load pickle for exports and imports of data  \n",
    "import pickle \n",
    "def load_obj(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def save_obj(obj, path ):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a07e756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9992"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path('../Resources/finished_data_dict.pkl')\n",
    "data_dict_import = load_obj(path)\n",
    "path = Path('../Resources/finished_key_list.csv')\n",
    "key_list_import_df = pd.read_csv(path,index_col=0)\n",
    "key_list = []\n",
    "for i in key_list_import_df['Symbol']:\n",
    "    key_list.append(i)\n",
    "len(key_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "017afc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float = 0 for 2112 symbols\n",
      "All list length: 3695\n",
      "ETF list length: 1872\n",
      "EQT list length: 1823\n"
     ]
    }
   ],
   "source": [
    "ftd_key_list = []\n",
    "etf_key_list = []\n",
    "eqt_key_list = []\n",
    "float_count_is_0 = 0\n",
    "\n",
    "for i in key_list:\n",
    "    index_value = i\n",
    "    float_ftd_pct_ytd = data_dict_import[index_value]['ftd_stats']['float_ftd_pct_ytd']\n",
    "    ## Check if there's float data. Otherwise, calculate using Outstanding share data\n",
    "    ## ETFs have no float data, so they always use Outstanding Shares \n",
    "    if float_ftd_pct_ytd == 0:        \n",
    "        os_ftd_pct_ytd = data_dict_import[index_value]['ftd_stats']['os_ftd_pct_ytd']\n",
    "        if os_ftd_pct_ytd >= 5:\n",
    "            float_count_is_0 += 1\n",
    "            ftd_key_list.append(i)\n",
    "            isEtf = data_dict_import[index_value]['companyProfile']['isEtf']\n",
    "            if isEtf == True:\n",
    "                etf_key_list.append(i)\n",
    "            else:\n",
    "                eqt_key_list.append(i)\n",
    "    else:\n",
    "        if float_ftd_pct_ytd >= 5:\n",
    "            ftd_key_list.append(i)\n",
    "            isEtf = data_dict_import[index_value]['companyProfile']['isEtf']\n",
    "            if isEtf == 'True':\n",
    "                etf_key_list.append(i)\n",
    "            else:\n",
    "                eqt_key_list.append(i)\n",
    "\n",
    "\n",
    "path = Path('../Resources/ftd_key_list_sorted.pkl')\n",
    "save_obj(ftd_key_list,path)             \n",
    "path = Path('../Resources/etf_key_list_sorted.pkl')\n",
    "save_obj(etf_key_list,path) \n",
    "path = Path('../Resources/eqt_key_list_sorted.pkl')\n",
    "save_obj(eqt_key_list,path) \n",
    "print(f'Float = 0 for '+str(float_count_is_0)+' symbols')\n",
    "print('All list length:',len(ftd_key_list))\n",
    "print('ETF list length:',len(etf_key_list))\n",
    "print('EQT list length:',len(eqt_key_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "091cea3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_symbol = 'GME'\n",
    "# symbol = test_symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "068e571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = data_dict_import[symbol]['dataFrame'].copy()\n",
    "# X = df.drop(columns={'close','adjClose'}).values\n",
    "# y = df['close'].values\n",
    "\n",
    "# # # Create training and testing datasets\n",
    "# # X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)\n",
    "\n",
    "# scaler = StandardScaler().fit(X)\n",
    "# X = scaler.transform(X) \n",
    "\n",
    "# # Create a sequential model\n",
    "# nn = Sequential()\n",
    "\n",
    "# num_of_inputs = 16\n",
    "# num_of_outputs= 1\n",
    "\n",
    "# # First hidden layer\n",
    "# nn.add(Dense(units=8, input_dim=num_of_inputs, activation=\"relu\"))\n",
    "\n",
    "# # Second hidden layer\n",
    "# nn.add(Dense(units=8, activation=\"relu\"))\n",
    "\n",
    "# # Output layer\n",
    "# nn.add(Dense(units=num_of_outputs, activation=\"linear\"))\n",
    "\n",
    "# # Compile the model\n",
    "# nn.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[\"mse\"])\n",
    "\n",
    "# # Fit the model\n",
    "# model = nn.fit(X, y, validation_split=0.3, epochs=800, verbose=0)\n",
    "\n",
    "# # Save model_2 as JSON\n",
    "# nn_json = nn.to_json()\n",
    "\n",
    "# file_path = Path('../Model_Data/'+symbol+'_model_2Layers.json')\n",
    "# with open(file_path, \"w\") as json_file:\n",
    "#     json_file.write(nn_json)\n",
    "\n",
    "# # Save weights\n",
    "# file_path = Path('../Model_Data/'+symbol+'_model_2Layers_weights.h5')\n",
    "# nn.save_weights(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24248dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load json and create model\n",
    "# file_path = Path('../Model_Data/'+symbol+'_model_2Layers.json')\n",
    "# with open(file_path, \"r\") as json_file:\n",
    "#     model_json = json_file.read()\n",
    "# loaded_model = model_from_json(model_json)\n",
    "\n",
    "# # load weights into new model\n",
    "# file_path = Path('../Model_Data/'+symbol+'_model_2Layers_weights.h5')\n",
    "# loaded_model.load_weights(file_path)\n",
    "\n",
    "# df2 = df.copy()\n",
    "# df2[\"predicted\"] = loaded_model.predict(X)\n",
    "# df_pred = df2[['close','predicted']]\n",
    "# df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "643931cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_loss, model_accuracy = nn.evaluate(X, y, verbose=2)\n",
    "# print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8c1f81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b8e02b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1823"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eqt_key_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7eadc47d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_key_list = eqt_key_list[0:10]\n",
    "len(test_key_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6976df20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, stock_list, validation_split_value, epochs_value, num_of_inputs, num_of_outputs):\n",
    "        \n",
    "        from tensorflow.keras.models import Sequential\n",
    "        from tensorflow.keras.layers import Dense\n",
    "        from tensorflow.keras.models import model_from_json\n",
    "        from pathlib import Path\n",
    "        import pandas as pd\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        \n",
    "        self.validation_split_value = validation_split_value\n",
    "        self.epochs_value = epochs_value\n",
    "        self.num_of_inputs = num_of_inputs\n",
    "        self.num_of_outputs = num_of_outputs\n",
    "        self.symbol_accuracy_dict = {}\n",
    "        \n",
    "        self.sequential = Sequential\n",
    "        self.Dense = Dense\n",
    "        self.model_from_json = model_from_json\n",
    "        self.Path = Path\n",
    "        self.pd = pd\n",
    "        self.train_test_split = train_test_split\n",
    "        self.StandardScalar = StandardScalar\n",
    "        self.stock_list = stock_list\n",
    "        \n",
    "        \n",
    "    def run_neural_network(self):\n",
    "        stock_list = self.stock_list\n",
    "        accurayc_dict = self.symbol_accuracy_dict\n",
    "        \n",
    "        for key in stock_list:\n",
    "            symbol = key\n",
    "            df = data_dict_import[symbol]['dataFrame'].copy()\n",
    "            X = df.drop(columns={'close','adjClose'}).values\n",
    "            y = df['close'].values\n",
    "            scaler = self.StandardScaler().fit(X)\n",
    "            X = scaler.transform(X) \n",
    "\n",
    "            ## Run Model 5 times and save best model \n",
    "            for i in range(5):\n",
    "                ## Initialize Model \n",
    "                nn = self.sequential()\n",
    "\n",
    "                # First hidden layer\n",
    "                nn.add(self.Dense(units=8, input_dim=num_of_inputs, activation=\"relu\"))\n",
    "\n",
    "                # Second hidden layer\n",
    "                nn.add(self.Dense(units=8, activation=\"relu\"))\n",
    "\n",
    "                # Output layer\n",
    "                nn.add(self.Dense(units=num_of_outputs, activation=\"linear\"))\n",
    "\n",
    "                # Compile the model\n",
    "                nn.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[\"mse\"])\n",
    "\n",
    "                # Fit the model\n",
    "                model = nn.fit(X, y, \n",
    "                               validation_split=validation_split_value, \n",
    "                               epochs=epochs_value, \n",
    "                               verbose=0)\n",
    "\n",
    "                model_loss, model_accuracy = nn.evaluate(X, y, verbose=0)\n",
    "\n",
    "                if i == 0:\n",
    "                    model_accuracy1 = model_accuracy\n",
    "                    symbol_accuracy_dict[symbol] = model_accuracy1\n",
    "                    # Save model as JSON\n",
    "                    nn_json = nn.to_json()\n",
    "\n",
    "                    file_path = Path('../Model_Data/'+symbol+'_model_2Layers.json')\n",
    "                    with open(file_path, \"w\") as json_file:\n",
    "                        json_file.write(nn_json)\n",
    "\n",
    "                    # Save weights\n",
    "                    file_path = Path('../Model_Data/'+symbol+'_model_2Layers_weights.h5')\n",
    "                    nn.save_weights(file_path)\n",
    "                else:\n",
    "                    if model_accuracy > model_accuracy1:\n",
    "                        model_accuracy1 = model_accuracy\n",
    "                        symbol_accuracy_dict[symbol] = model_accuracy1\n",
    "                        # Save model as JSON\n",
    "                        nn_json = nn.to_json()\n",
    "\n",
    "                        file_path = Path('../Model_Data/'+symbol+'_model_2Layers.json')\n",
    "                        with open(file_path, \"w\") as json_file:\n",
    "                            json_file.write(nn_json)\n",
    "\n",
    "                        # Save weights\n",
    "                        file_path = Path('../Model_Data/'+symbol+'_model_2Layers_weights.h5')\n",
    "                        nn.save_weights(file_path)\n",
    "        path = Path('../Resources/symbol_accuracy_dict.pkl')\n",
    "        save_obj(symbol_accuracy_dict,path) \n",
    "        \n",
    "    def load_model(self, symbol):\n",
    "        path = self.Path('../FilesExport_Finished/'+symbol+'_finished_df.pkl')\n",
    "        data_import = load_obj(path)\n",
    "        df = data_import['dataFrame'].copy()\n",
    "        \n",
    "        X = df.drop(columns={'close','adjClose'}).values\n",
    "        y = df['close'].values\n",
    "\n",
    "        scaler = StandardScaler().fit(X)\n",
    "        X = scaler.transform(X) \n",
    "        \n",
    "        with open(file_path, \"r\") as json_file:\n",
    "            model_json = json_file.read()\n",
    "        loaded_model = model_from_json(model_json)\n",
    "        \n",
    "        file_path = Path('../Model_Data/'+symbol+'_model_2Layers_weights.h5')\n",
    "        loaded_model.load_weights(file_path)\n",
    "        loaded_model.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[\"mse\"])\n",
    "        score = loaded_model.evaluate(X, y, verbose=0)\n",
    "        print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n",
    "        \n",
    "        df2 = df.copy()\n",
    "        df2['predicted'] = loaded_model.predict(X)\n",
    "        df_pred = df2[['close','predicted']]\n",
    "        \n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96c3f2e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-601d4b2d500c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m                        \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_split_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m                        \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m                        verbose=0)\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mmodel_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mlenv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mlenv\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1261\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m               \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1263\u001b[1;33m               _use_cached_eval_dataset=True)\n\u001b[0m\u001b[0;32m   1264\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'val_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mlenv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mlenv\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1529\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_test_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1530\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1531\u001b[1;33m       \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Single epoch.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1532\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1533\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mlenv\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1193\u001b[0m     \u001b[1;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1194\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1195\u001b[1;33m       \u001b[0mdata_iterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1196\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1197\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    488\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 490\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    491\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m       raise RuntimeError(\"`tf.data.Dataset` only supports Python-style \"\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    724\u001b[0m             \u001b[1;34m\"When `dataset` is provided, `element_spec` and `components` must \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    725\u001b[0m             \"not be specified.\")\n\u001b[1;32m--> 726\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    727\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_next_call_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    749\u001b[0m               \u001b[0moutput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[1;32m--> 751\u001b[1;33m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    752\u001b[0m       \u001b[1;31m# Delete the resource when this object is deleted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m       self._resource_deleter = IteratorResourceDeleter(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   3235\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3236\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m-> 3237\u001b[1;33m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[0m\u001b[0;32m   3238\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3239\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "validation_split_value=0.3\n",
    "epochs_value=400\n",
    "num_of_inputs = 16\n",
    "num_of_outputs= 1\n",
    "\n",
    "##    Save accuracy values for each symbol, \n",
    "##    so we can find the best models to load afterwards\n",
    "symbol_accuracy_dict = {}\n",
    "\n",
    "for key in test_key_list:\n",
    "    symbol = key\n",
    "    df = data_dict_import[symbol]['dataFrame'].copy()\n",
    "    X = df.drop(columns={'close','adjClose'}).values\n",
    "    y = df['close'].values\n",
    "    scaler = StandardScaler().fit(X)\n",
    "    X = scaler.transform(X) \n",
    "    \n",
    "    ## Run Model 5 times and save best model \n",
    "    for i in range(5):\n",
    "        ## Initialize Model \n",
    "        nn = Sequential()\n",
    "        \n",
    "        # First hidden layer\n",
    "        nn.add(Dense(units=8, input_dim=num_of_inputs, activation=\"relu\"))\n",
    "\n",
    "        # Second hidden layer\n",
    "        nn.add(Dense(units=8, activation=\"relu\"))\n",
    "\n",
    "        # Output layer\n",
    "        nn.add(Dense(units=num_of_outputs, activation=\"linear\"))\n",
    "\n",
    "        # Compile the model\n",
    "        nn.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[\"mse\"])\n",
    "\n",
    "        # Fit the model\n",
    "        model = nn.fit(X, y, \n",
    "                       validation_split=validation_split_value, \n",
    "                       epochs=epochs_value, \n",
    "                       verbose=0)\n",
    "    \n",
    "        model_loss, model_accuracy = nn.evaluate(X, y, verbose=0)\n",
    "\n",
    "        if i == 0:\n",
    "            model_accuracy1 = model_accuracy\n",
    "            symbol_accuracy_dict[symbol] = model_accuracy1\n",
    "            # Save model as JSON\n",
    "            nn_json = nn.to_json()\n",
    "\n",
    "            file_path = Path('../Model_Data/'+symbol+'_model_2Layers.json')\n",
    "            with open(file_path, \"w\") as json_file:\n",
    "                json_file.write(nn_json)\n",
    "\n",
    "            # Save weights\n",
    "            file_path = Path('../Model_Data/'+symbol+'_model_2Layers_weights.h5')\n",
    "            nn.save_weights(file_path)\n",
    "        else:\n",
    "            if model_accuracy > model_accuracy1:\n",
    "                model_accuracy1 = model_accuracy\n",
    "                symbol_accuracy_dict[symbol] = model_accuracy1\n",
    "                # Save model as JSON\n",
    "                nn_json = nn.to_json()\n",
    "\n",
    "                file_path = Path('../Model_Data/'+symbol+'_model_2Layers.json')\n",
    "                with open(file_path, \"w\") as json_file:\n",
    "                    json_file.write(nn_json)\n",
    "\n",
    "                # Save weights\n",
    "                file_path = Path('../Model_Data/'+symbol+'_model_2Layers_weights.h5')\n",
    "                nn.save_weights(file_path)\n",
    "\n",
    "                \n",
    "#     # Save model as JSON\n",
    "#     nn_json = nn.to_json()\n",
    "\n",
    "#     file_path = Path('../Model_Data_Temp/Temp_model_2Layers.json')\n",
    "#     with open(file_path, \"w\") as json_file:\n",
    "#         json_file.write(nn_json)\n",
    "\n",
    "#     # Save weights\n",
    "#     file_path = Path('../Model_Data_Temp/Temp_model_2Layers_weights.h5')\n",
    "#     nn.save_weights(file_path)\n",
    "    \n",
    "#     # load json and create model\n",
    "#     file_path = Path('../Model_Data/Temp_model_2Layers.json')\n",
    "#     with open(file_path, \"r\") as json_file:\n",
    "#         model_json = json_file.read()\n",
    "#     loaded_model = model_from_json(model_json)\n",
    "\n",
    "#     # load weights into new model\n",
    "#     file_path = Path('../Model_Data/Temp_model_2Layers_weights.h5')\n",
    "#     loaded_model.load_weights(file_path)\n",
    "\n",
    "#     df2 = df.copy()\n",
    "#     df2[\"predicted\"] = loaded_model.predict(X)\n",
    "#     df_pred = df2[['close','predicted']]\n",
    "\n",
    "\n",
    "\n",
    "## Export symbol_accuracy_dict\n",
    "path = Path('../Resources/symbol_accuracy_dict.pkl')\n",
    "save_obj(symbol_accuracy_dict,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4045d47c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feca13b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
