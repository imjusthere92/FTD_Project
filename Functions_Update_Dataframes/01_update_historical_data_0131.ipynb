{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66beb0bb-7968-4e19-80f3-c00dc8976cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "\n",
    "import quandl\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2616231-8652-42d9-a5ed-b23f36a4b4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For TI data \n",
    "from datetime import date\n",
    "from scipy import stats\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "from numpy.lib import pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f5ab285-48dc-487b-ad5c-7fe0cfbdc38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use pickle module to import and export and save files\n",
    "import pickle\n",
    "def load_obj(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "def save_obj(obj, path ):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0814e75-b2ff-42e7-81bd-2e8d37dccfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Constants \n",
    "\n",
    "## Set start date variable - api dataframes will be created starting from this date\n",
    "\n",
    "start_date = '2022-01-01'   ## Use existing dataframes, and add new data to them \n",
    "end_date = '2022-01-31'     ## Use end of January - will attempt to predict up to first 10 days \n",
    "                            ## of trading for February\n",
    "    \n",
    "export_date = '01_31'       ## For folder path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b06e48d-9b21-483a-ac80-26b54a91b5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get Short \"Interest\" Data from Quandl \n",
    "## QUANDL/NASDAQ \n",
    "nsdq_api_key = os.environ.get('NASDAQ_API_KEY')\n",
    "base_url_nsdq = 'https://data.nasdaq.com/api/v3/datasets/FINRA/'\n",
    "\n",
    "def get_short_data_QUANDL(symbol):\n",
    "    string_nsdq = \"FINRA/FNSQ_\"+symbol\n",
    "    string_nyse = \"FINRA/FNYX_\"+symbol\n",
    "    \n",
    "    df1 = quandl.get(string_nsdq,start_date=start_date,end_date=end_date,authtoken=nsdq_api_key)   ## Nasdaq\n",
    "    df2 = quandl.get(string_nyse,start_date=start_date,end_date=end_date,authtoken=nsdq_api_key)   ## NYSE\n",
    "\n",
    "    df1 = df1.rename(columns={'ShortVolume':'ShortVolumeNSDQ','TotalVolume':'TotalVolumeNSDQ'})\n",
    "    #df1 = df1.drop(columns={'ShortExemptVolume'})\n",
    "    df1 = df1.rename(columns={'ShortExemptVolume':'ShortExemptVolumeNSDQ'})\n",
    "\n",
    "    df2 = df2.rename(columns={'ShortVolume':'ShortVolumeNYSE','TotalVolume':'TotalVolumeNYSE'})\n",
    "    #df2 = df2.drop(columns={'ShortExemptVolume'})\n",
    "    df2 = df2.rename(columns={'ShortExemptVolume':'ShortExemptVolumeNYSE'})\n",
    "\n",
    "    df3 = pd.merge(df1,df2,on='Date',how='outer')\n",
    "    #df3 = df3.fillna(0)\n",
    "    \n",
    "    return df3\n",
    "\n",
    "\n",
    "## Not needed \n",
    "\n",
    "# ## Return FTD Data from SEC FTD files using a Stock's CUSIP number to sort \n",
    "# def return_ftd_data_cusip(cusip_number):\n",
    "#     df = ftd_df.copy()\n",
    "#     df.set_index(\"CUSIP\",inplace=True)\n",
    "#     df = df.loc[cusip_number]\n",
    "#     df['Date'] = pd.to_datetime(df['Date'])\n",
    "#     df = df.set_index('Date')\n",
    "#     return df\n",
    "\n",
    "# ## Return the CUSIP symbol from the symbol_df symbol list \n",
    "# def return_CUSIP_from_symbol(symbol):\n",
    "#     df = symbol_df.copy()\n",
    "#     df.set_index('SYMBOL',inplace=True)\n",
    "#     cusip_variable = df.loc[symbol]\n",
    "#     cusip_variable = cusip_variable['CUSIP']\n",
    "#     return cusip_variable\n",
    "\n",
    "# def return_ftd_data_symbol(symbol):\n",
    "#     cusip_number = return_CUSIP_from_symbol(symbol)\n",
    "#     df = return_ftd_data_cusip(cusip_number)\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa8286db-f1eb-4492-a532-f5410616782e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FMP Constants \n",
    "fmpbase_urlv3 = 'https://fmpcloud.io/api/v3/'\n",
    "fmpbase_urlv4 = 'https://fmpcloud.io/api/v4/'\n",
    "api_key = os.getenv(\"FMP_CLOUD_API_KEY\")\n",
    "\n",
    "## FMP Functions \n",
    "def get_FMP_historical_data(symbol, startDate=start_date, endDate=end_date, apiKey=api_key):\n",
    "    url_hist_price = fmpbase_urlv3+'historical-price-full/'\n",
    "    url_hist_query_with_date = url_hist_price+symbol+'?from='+startDate+'&to='+endDate+'&apikey='+apiKey\n",
    "    resp_data = requests.get(url_hist_query_with_date)\n",
    "    json_ = resp_data.json()\n",
    "    data = json_['historical']\n",
    "    df = pd.DataFrame(data)\n",
    "    df.rename(columns={'date':'Date'},inplace=True)\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df = df.reindex(index=df.index[::-1]) ## Reverse the DataFrame \n",
    "    df.set_index('Date',inplace=True)\n",
    "    df.drop(columns='label',inplace=True)\n",
    "    return df\n",
    "\n",
    "# def get_float_data_FMP(symbol):\n",
    "#     url_float_shares = fmpbase_urlv4+'shares_float?symbol='\n",
    "#     url_query_float_data = url_float_shares+symbol+'&apikey='+api_key\n",
    "#     resp_data = requests.get(url_query_float_data)\n",
    "#     #df = pd.DataFrame(resp_data.json())\n",
    "#     json_ = resp_data.json()\n",
    "#     return json_[0]\n",
    "\n",
    "# def get_company_profile_FMP_json(symbol):\n",
    "#     ## https://fmpcloud.io/api/v3/profile/AAPL?apikey='yourkeyhere'\n",
    "#     url_company_profile_url = fmpbase_urlv3+'profile/'+symbol+'?apikey='+api_key\n",
    "#     resp_data = requests.get(url_company_profile_url)\n",
    "#     json_response = resp_data.json()\n",
    "#     return json_response[0]\n",
    "\n",
    "# # def save_and_export_raw_df_csv(data, symbol, path='None'):\n",
    "# #     ## Can set custom path (useful for testing) otherwise will default to below path\n",
    "# #     if path=='None':\n",
    "# #         path = ('../FilesExportIndividualStockDFs_Big/'+symbol+'_combined_df.csv')\n",
    "# #     data.to_csv(path)\n",
    "    \n",
    "# def save_and_export_raw_df_pkl(data, symbol, path='None'):\n",
    "#     ## Can set custom path (useful for testing) otherwise will default to below path\n",
    "#     if path=='None':\n",
    "#         path = ('../FilesExport_TimeSeries_DFs/'+symbol+'_combined_df.pkl')\n",
    "#     save_obj(data,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a4c6d84-c4a9-4a0e-b379-33ed68a5403d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_data(symbol):\n",
    "    \n",
    "    ## Get updated data\n",
    "    df = get_FMP_historical_data(symbol)    \n",
    "    df2 = get_short_data_QUANDL(symbol)\n",
    "    df = pd.merge(df,df2,on='Date',how='outer')\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    ## Import old dataframe, drop FTDs, join data and save \n",
    "    path = Path('../FilesExport_Complete_DFs_TI_noShift/'+symbol+'_TI_DF_no_shift.pkl')\n",
    "    data = load_obj(path)\n",
    "    import_df = data[symbol].drop(columns='QUANTITY_FAILS')\n",
    "    \n",
    "    updated_df = import_df.append(df)\n",
    "    \n",
    "    return updated_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "729e8400-fabf-4ad4-b66f-61e8c9f4433a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup TI functions \n",
    "\n",
    "def get_dataframe(symbol):\n",
    "\n",
    "    #data = pd.read_csv(\"../FilesExportIndividualStockDFs_Big/\"+ticker+\"_combined_df.csv\", index_col='Date', parse_dates=True)\n",
    "    # path = Path('../FilesExport_Complete_DataDicts/'+symbol+'_data_dict.pkl')\n",
    "    # data_import = load_obj(path)\n",
    "    # data = data_import[symbol]['dataFrame']\n",
    "\n",
    "    ## Use newly made function to get_dataframe \n",
    "    \n",
    "    data = update_data(symbol)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def rolling_spearman(seqa, seqb, window):\n",
    "    stridea = seqa.values.strides[0]\n",
    "    ssa = as_strided(seqa, shape=[len(seqa) - window + 1, window], strides=[stridea, stridea])\n",
    "    strideb = seqa.values.strides[0]\n",
    "    ssb = as_strided(seqb, shape=[len(seqb) - window + 1, window], strides =[strideb, strideb])\n",
    "    ar = pd.DataFrame(ssa)\n",
    "    br = pd.DataFrame(ssb)\n",
    "    ar = ar.rank(1)\n",
    "    br = br.rank(1)\n",
    "    corrs = ar.corrwith(br, 1)\n",
    "    return pad(corrs, (window - 1, 0), 'constant', constant_values=np.nan)\n",
    "\n",
    "def bollinger_bands(dataframe,period=20):\n",
    "    data = dataframe.copy()\n",
    "    data['middle_band'] = data[['adjClose']].rolling(window=period).mean()\n",
    "    data[str(period)+'_day_stdev'] = data[['adjClose']].rolling(window=period).std()\n",
    "    data['upper_band'] = data['middle_band']+2*data[str(period)+'_day_stdev']\n",
    "    data['lower_band'] = data['middle_band'] - 2*data[str(period)+'_day_stdev']\n",
    "    data['spread'] = data['upper_band'] + data['lower_band']\n",
    "    data['change_in_spread'] = data['spread']/data['spread'].shift(1)-1\n",
    "    data[str(period)+\"_return\"] = data['adjClose']/data['adjClose'].shift(period)-1\n",
    "    data['bollinger_signal'] = data['change_in_spread'].rank(ascending=False, pct=True)\n",
    "    data.dropna()\n",
    "\n",
    "    return data\n",
    "\n",
    "def dema(dataframe, period1=10, period2=20):\n",
    "    data = dataframe.copy()\n",
    "    data[str(period1)+'ema1'] = dataframe[['adjClose']].ewm(span=period1, adjust=False).mean()\n",
    "    data[str(period1)+'ema2'] = data[str(period1)+'ema1'].ewm(span=period1, adjust=False).mean()\n",
    "    data[str(period1)+'dema'] = 2*data[str(period1)+'ema1'] - data[str(period1)+'ema2']\n",
    "    data[str(period2)+'ema1'] = data[['adjClose']].ewm(span=period2, adjust=False).mean()\n",
    "    data[str(period2)+'ema2'] = data[str(period2)+'ema1'].ewm(span=period2, adjust=False).mean()\n",
    "    data[str(period2)+'dema'] = 2*data[str(period2)+'ema1'] - data[str(period2)+'ema2']\n",
    "    data[str(period1)+\"_return\"] = data['adjClose']/data['adjClose'].shift(period1)-1\n",
    "    data['spread'] = data[str(period1)+'dema'] - data[str(period2)+'dema']\n",
    "    data['dema_signal'] = data['spread'].rank(ascending=True, pct=True)\n",
    "    #data = data.dropna()\n",
    "    return data\n",
    "\n",
    "def price_momentum(dataframe, smoothing1=0.0571, smoothing2=0.1, periods1=15, periods2=10):\n",
    "    data = dataframe.copy()\n",
    "    data['smoothing_factor'] = smoothing1\n",
    "    data[str(periods1)+\"average\"] = data['changeOverTime'].rolling(window=periods1).mean()\n",
    "    smoothing_factor_list = [data.iloc[periods1][str(periods1)+\"average\"]]\n",
    "    data = data.dropna()\n",
    "    i=1\n",
    "    j=0\n",
    "    while i < len(data[str(periods1)+\"average\"]):\n",
    "        smoothing_factor = data.iloc[i]['changeOverTime']*data.iloc[i]['smoothing_factor'] + smoothing_factor_list[j]*(1-data.iloc[i]['smoothing_factor'])\n",
    "        smoothing_factor_list.append(smoothing_factor)\n",
    "        j+=1\n",
    "        i+=1\n",
    "    data['35d_custom_smoothing'] = smoothing_factor_list\n",
    "    data['35d_custom_10'] = data['35d_custom_smoothing']*10\n",
    "    data['smoothing_factor2'] = smoothing2\n",
    "    data[str(periods2)+\"average\"] = data['35d_custom_10'].rolling(window=periods2).mean()\n",
    "    data = data.dropna()\n",
    "    smoothing_factor_list2 = [data.iloc[0][str(periods2)+\"average\"]]\n",
    "    i=1\n",
    "    j=0\n",
    "    while i < len(data[str(periods2)+\"average\"]):\n",
    "        smoothing_factor = (data.iloc[i]['35d_custom_10'] - smoothing_factor_list2[j])*data.iloc[i]['smoothing_factor2'] + smoothing_factor_list2[j]\n",
    "        smoothing_factor_list2.append(smoothing_factor)\n",
    "        j+=1\n",
    "        i+=1\n",
    "    data[str(periods2)+'d_custom_smoothing'] = smoothing_factor_list2\n",
    "    data[str(periods2)+\"_return\"] = data['adjClose']/data['adjClose'].shift(periods2)-1\n",
    "#         data['signal'] = np.where(data[str(periods2)+'d_custom_smoothing'] > data[str(periods2)+'d_custom_smoothing'].shift(1), 1.0, 0.0)\n",
    "#         data = data.rename(columns={'signal':'price_mo'})\n",
    "    return data\n",
    "\n",
    "def get_ichimoku_cloud(dataframe, period1=4, period2=8, period3=15):\n",
    "\n",
    "    #TODO generate signal, ichimoku works better in current market regime with shorter periods, being able to respond faster to events than a traditional version\n",
    "    # The conversion crossing the base would be the signal\n",
    "\n",
    "    data = dataframe.copy()\n",
    "    data['conversion_line'] = data[['adjClose']].rolling(window=period1).mean()\n",
    "    data['base_line'] = data[['adjClose']].rolling(window=period2).mean()\n",
    "    data['senkou_spanA_line'] = (data['conversion_line']+data['base_line'])/2\n",
    "    data['senkou_spanB_line'] = data[['adjClose']].rolling(window=period3).mean()\n",
    "    data['lagging_span'] = data['adjClose'].shift(period2)\n",
    "    data = data.dropna()\n",
    "\n",
    "    return data\n",
    "\n",
    "def accumulation_distribution_line(dataframe):\n",
    "\n",
    "    ##TODO define periodicity and pass as arguments, use the mean as the signal generator, -1 is buy and and 1 is sell\n",
    "    ##TODO need to add ability to ignore a -1 during a range of 1s\n",
    "\n",
    "    data = dataframe.copy()\n",
    "    data['money_flow_mult'] = round(((data['adjClose'] - data['low']) - (data['high'] - data['adjClose']))/(data['high'] - data['low']),2)\n",
    "    #data = data.dropna()\n",
    "    data['money_flow_volume'] = data['money_flow_mult']*data['volume']\n",
    "    money_flow_multiplier_list = list(data['money_flow_volume'].values)\n",
    "    adl = [money_flow_multiplier_list[0]]\n",
    "    i = 1\n",
    "    while i < len(money_flow_multiplier_list):\n",
    "        a_d_indicator = adl[i-1]+money_flow_multiplier_list[i]\n",
    "        adl.append(a_d_indicator)\n",
    "        i+=1\n",
    "    data['adl'] = adl\n",
    "    data['adl_change'] = data['adl']/data['adl'].shift(1)-1\n",
    "    negative_change_count = [0]*9\n",
    "    i = 0\n",
    "    counter = 0\n",
    "    while i < len(data)-9:\n",
    "        j=0\n",
    "        while j < 9:\n",
    "            if data.iloc[j+i]['adl_change'] <0:\n",
    "                counter+=1\n",
    "            if j %19 == 0:\n",
    "                negative_change_count.append(counter)\n",
    "                counter = 0\n",
    "            j+=1\n",
    "        i+=1\n",
    "    data['negative_change_counter'] = negative_change_count\n",
    "    data['9_day_return'] = data['adjClose']/data['adjClose'].shift(9)-1\n",
    "    data['adl_signal'] = rolling_spearman(data['adl'], data['9_day_return'], 9)\n",
    "\n",
    "    return data\n",
    "\n",
    "def rsi(dataframe, periods=14):\n",
    "    data = dataframe.copy()\n",
    "    data['gains'] = np.where(data['changeOverTime']>0, data['changeOverTime'], 0)\n",
    "    data['losses'] = np.where(data['changeOverTime']<0, np.absolute(data['changeOverTime']), 0)\n",
    "    data['average_gain'] = data['gains'].rolling(window=periods).mean()\n",
    "    data['average_loss'] = data['losses'].rolling(window=periods).mean()\n",
    "    data['rs'] = data['average_gain']/data['average_loss']\n",
    "    data['rsi'] = (100 - 100/(1+data['rs']))\n",
    "    data['rsi_signal'] = data['rsi'].rank(ascending=True, pct=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "def get_all_indicators(symbol):\n",
    "    dataframe = get_dataframe(symbol)\n",
    "    bb = bollinger_bands(dataframe)\n",
    "    DEMA = dema(dataframe)\n",
    "    ADL = accumulation_distribution_line(dataframe)\n",
    "    RSI = rsi(dataframe)\n",
    "\n",
    "    dataframe['bollinger_signal'] = bb['bollinger_signal']\n",
    "    dataframe['dema_signal'] = DEMA['dema_signal']\n",
    "    dataframe['adl_signal'] = ADL['adl_signal']\n",
    "    dataframe['rsi_signal'] = RSI['rsi_signal']\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e4d48b4-3518-43ca-a4e4-aca9034817d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Steps to take :\n",
    "## Get updated_df\n",
    "## Run TI statistics on new dataframe \n",
    "## Export new, updated dataframe for use in new models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c1a3a21-0046-4bbe-9493-41c1ed8dde39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "770"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Import machine learning list \n",
    "path = Path('../Resources/06_01_ML_symbol_success_list.pkl')\n",
    "machine_learning_dict = load_obj(path)\n",
    "len(machine_learning_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ce08170-8c8c-4c79-9a2a-c2ab41b908fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "767"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbol_success_list = []\n",
    "for symbol in machine_learning_dict:\n",
    "    temp_dict = {}\n",
    "    try:\n",
    "        df = get_all_indicators(symbol)\n",
    "    except: continue \n",
    "        \n",
    "    df = df.dropna()\n",
    "    \n",
    "    if len(df) > 1400:\n",
    "        temp_dict[symbol] = df\n",
    "        #data_dict[symbol] = df \n",
    "        symbol_success_list.append(symbol)\n",
    "        \n",
    "        ## Export Temp dict \n",
    "        export_path = Path('../FilesExport_Updated_DFs_'+export_date+'/'+symbol+'_ti_df_no_ftd.pkl')\n",
    "        save_obj(temp_dict,export_path)\n",
    "        \n",
    "        \n",
    "len(symbol_success_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d04b2615-773a-41fe-804c-aaa9664da74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#symbol_success_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8cda8301-4158-4eff-993f-b726ebaaa8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Export symbol_success_list as ML_symbol_success_list.pkl\n",
    "export_path = Path('../Resources/Updated_01_DF_success_list_'+export_date+'.pkl')\n",
    "save_obj(symbol_success_list,export_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c34f2ac-95ad-4888-a488-7a3a85682de1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
