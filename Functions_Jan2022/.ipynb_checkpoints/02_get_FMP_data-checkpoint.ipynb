{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e05a692-4c5e-490a-9527-cd9f28b56181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "\n",
    "import quandl\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5d8f201-1904-4bd5-93fa-6fa89fc8c6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Constants \n",
    "\n",
    "## Set start date variable - dataframes will be created starting from this date\n",
    "start_date = '2016-01-01'\n",
    "end_date = '2021-10-29'\n",
    "default_date_range = '71m' ## Default Range for IEX functions - don't need more at the moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c84aaab-7cfe-4045-bf07-f60150266539",
   "metadata": {},
   "outputs": [],
   "source": [
    "## QUANDL/NASDAQ \n",
    "nsdq_api_key = os.environ.get('NASDAQ_API_KEY')\n",
    "base_url_nsdq = 'https://data.nasdaq.com/api/v3/datasets/FINRA/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a100c029-82b5-4b39-a710-21103d95e396",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get Short \"Interest\" Data from Quandl \n",
    "def get_short_data_QUANDL(symbol):\n",
    "    string_nsdq = \"FINRA/FNSQ_\"+symbol\n",
    "    string_nyse = \"FINRA/FNYX_\"+symbol\n",
    "    \n",
    "    df1 = quandl.get(string_nsdq,start_date=start_date,end_date=end_date,authtoken=nsdq_api_key)   ## Nasdaq\n",
    "    df2 = quandl.get(string_nyse,start_date=start_date,end_date=end_date,authtoken=nsdq_api_key)   ## NYSE\n",
    "\n",
    "    df1 = df1.rename(columns={'ShortVolume':'ShortVolumeNSDQ','TotalVolume':'TotalVolumeNSDQ'})\n",
    "    #df1 = df1.drop(columns={'ShortExemptVolume'})\n",
    "    df1 = df1.rename(columns={'ShortExemptVolume':'ShortExemptVolumeNSDQ'})\n",
    "\n",
    "    df2 = df2.rename(columns={'ShortVolume':'ShortVolumeNYSE','TotalVolume':'TotalVolumeNYSE'})\n",
    "    #df2 = df2.drop(columns={'ShortExemptVolume'})\n",
    "    df2 = df2.rename(columns={'ShortExemptVolume':'ShortExemptVolumeNYSE'})\n",
    "\n",
    "    df3 = pd.merge(df1,df2,on='Date',how='outer')\n",
    "    #df3 = df3.fillna(0)\n",
    "    \n",
    "    return df3\n",
    "\n",
    "\n",
    "## Return FTD Data from SEC FTD files using a Stock's CUSIP number to sort \n",
    "def return_ftd_data_cusip(cusip_number):\n",
    "    df = ftd_df.copy()\n",
    "    df.set_index(\"CUSIP\",inplace=True)\n",
    "    df = df.loc[cusip_number]\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df = df.set_index('Date')\n",
    "    return df\n",
    "\n",
    "## Return the CUSIP symbol from the symbol_df symbol list \n",
    "def return_CUSIP_from_symbol(symbol):\n",
    "    df = symbol_df.copy()\n",
    "    df.set_index('SYMBOL',inplace=True)\n",
    "    cusip_variable = df.loc[symbol]\n",
    "    cusip_variable = cusip_variable['CUSIP']\n",
    "    return cusip_variable\n",
    "\n",
    "def return_ftd_data_symbol(symbol):\n",
    "    cusip_number = return_CUSIP_from_symbol(symbol)\n",
    "    df = return_ftd_data_cusip(cusip_number)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "004def6e-e654-48dd-9daa-579906cd6f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FMP Constants \n",
    "fmpbase_urlv3 = 'https://fmpcloud.io/api/v3/'\n",
    "fmpbase_urlv4 = 'https://fmpcloud.io/api/v4/'\n",
    "api_key = os.getenv(\"FMP_CLOUD_API_KEY\")\n",
    "\n",
    "## FMP Functions \n",
    "def get_FMP_historical_data(symbol, startDate=start_date, endDate=end_date, apiKey=api_key):\n",
    "    url_hist_price = fmpbase_urlv3+'historical-price-full/'\n",
    "    url_hist_query_with_date = url_hist_price+symbol+'?from='+startDate+'&to='+endDate+'&apikey='+apiKey\n",
    "    resp_data = requests.get(url_hist_query_with_date)\n",
    "    json_ = resp_data.json()\n",
    "    data = json_['historical']\n",
    "    df = pd.DataFrame(data)\n",
    "    df.rename(columns={'date':'Date'},inplace=True)\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df = df.reindex(index=df.index[::-1]) ## Reverse the DataFrame \n",
    "    df.set_index('Date',inplace=True)\n",
    "    df.drop(columns='label',inplace=True)\n",
    "    return df\n",
    "\n",
    "api_key = os.getenv(\"FMP_CLOUD_API_KEY\")\n",
    "def get_float_data_FMP(symbol):\n",
    "    url_float_shares = fmpbase_urlv4+'shares_float?symbol='\n",
    "    url_query_float_data = url_float_shares+symbol+'&apikey='+api_key\n",
    "    resp_data = requests.get(url_query_float_data)\n",
    "    #df = pd.DataFrame(resp_data.json())\n",
    "    json_ = resp_data.json()\n",
    "    return json_[0]\n",
    "\n",
    "def get_company_profile_FMP_json(symbol):\n",
    "    ## https://fmpcloud.io/api/v3/profile/AAPL?apikey='yourkeyhere'\n",
    "    url_company_profile_url = fmpbase_urlv3+'profile/'+symbol+'?apikey='+api_key\n",
    "    resp_data = requests.get(url_company_profile_url)\n",
    "    json_response = resp_data.json()\n",
    "    return json_response[0]\n",
    "\n",
    "def save_and_export_raw_df_csv(data, symbol):\n",
    "    path = ('../FilesExportIndividualStockDFs_Big/'+symbol+'_combined_df.csv')\n",
    "    data.to_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2be0b1b-be26-4e5a-959e-480fdd9807a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_series_data(symbol):\n",
    "    ftd_data = return_ftd_data_symbol(symbol)\n",
    "    ftd_data = ftd_data.drop(columns={'SYMBOL'})\n",
    "\n",
    "    fmp_data = get_FMP_historical_data(symbol)\n",
    "    df1 = pd.merge(fmp_data,ftd_data, on='Date',how='outer')\n",
    "    df1['QUANTITY_FAILS'] = df1['QUANTITY_FAILS'].fillna(0)\n",
    "    df1['volume'] = df1['volume'].fillna(0)\n",
    "    df1['unadjustedVolume'] = df1['unadjustedVolume'].fillna(0)\n",
    "    df1['vwap'] = df1['vwap'].fillna(0)\n",
    "    df2 = get_short_data_QUANDL(symbol)\n",
    "    df = pd.merge(df1,df2,on='Date',how='outer')\n",
    "    df = df.fillna(0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d44dcab8-29db-49e7-8f04-c3080c1dcc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use pickle module to import and export and save files\n",
    "import pickle\n",
    "def load_obj(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "def save_obj(obj, path ):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d72aee8-d91a-42ff-b8b0-a31a036f4c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import FTD File using CSV \n",
    "ftd_df = pd.read_csv(\n",
    "    Path('../Resources/ftd_all_data.csv'),\n",
    "    index_col=0, parse_dates=True\n",
    ")\n",
    "## Import Symbol and CUSIP list using CSV\n",
    "symbol_df = pd.read_csv(\n",
    "    Path('../Resources/symbol_all_list.csv'),\n",
    "    index_col=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7135d89b-e904-41ed-9ee3-7a07a290b2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set iteration through symbol_df \n",
    "length_ = len(symbol_df)\n",
    "\n",
    "complete_dict = {}            ## Empty dicts in order to create\n",
    "incomplete_dict = {}          ## two big files at end of while loop\n",
    "error_symbol_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31408711-1dca-4cb8-a2a0-8a31f24540ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create while loop that access FMP and gets data \n",
    "x = 0    \n",
    "y = 50\n",
    "increment = 50  ## Make sure increment is right\n",
    "test_length = 200 \n",
    "\n",
    "## Setup in 50 increments to find error in case the loop fails. \n",
    "\n",
    "\n",
    "loop_length = length_ ## Set to length_ to run as full, or your test_length\n",
    "\n",
    "## If not testing make x <= test_length\n",
    "while (x < loop_length):\n",
    "    ## Initialize temp dictionaries \n",
    "    complete_dict_temp = {}\n",
    "    incomplete_dict_temp = {}\n",
    "    ## Set y value for end of while loop\n",
    "    if y >= length_: y = length_\n",
    "    ## In theory this should stop the for loop from crashing \n",
    "    ##    at the end and running out of index range     \n",
    "    \n",
    "    range_var = range(x,y)\n",
    "    ## Set label variables for exports \n",
    "    str_symbol1 = str(x)\n",
    "    y2 = y - 1 \n",
    "    str_symbol2 = str(y2)\n",
    "    \n",
    "    for i in range_var: \n",
    "        ## Iterate through symbol list and create data by symbol\n",
    "        symbol_var = symbol_df['SYMBOL'][i]\n",
    "        \n",
    "        # Use while running loop, if error is made, can see what index it happened on \n",
    "        # can also check current value of symbol_var as well \n",
    "        error_var = i\n",
    "        \n",
    "        ## Get Time Series Data \n",
    "        try:\n",
    "            time_series_df = get_time_series_data(symbol_var)\n",
    "        except KeyError:## Some symbols fail and are unreadable. Unreadable symbols are unimportant and okay to be discarded\n",
    "            error_symbol_list.append(symbol_var)\n",
    "            continue\n",
    "        except: \n",
    "            error_symbol_list.append(symbol_var)\n",
    "            continue\n",
    "#         except AttributeError:\n",
    "#             continue\n",
    "#         except NotFoundError: ## Occurs when no Quandl data is found\n",
    "#             continue\n",
    "#         except NameError: ## Occurs when no Quandl data is found \n",
    "#             continue \n",
    "        ## If successful, export data \n",
    "        save_and_export_raw_df_csv(time_series_df,symbol_var)\n",
    "            \n",
    "        ## Check if null values, add to different dicts if null values present, or no nulls present\n",
    "        bool_var = time_series_df.isnull().values.any()\n",
    "        if bool_var == False:\n",
    "            complete_dict[symbol_var] = time_series_df\n",
    "            complete_dict_temp[symbol_var] = time_series_df\n",
    "        elif bool_var == True:\n",
    "            incomplete_dict[symbol_var] = time_series_df\n",
    "            incomplete_dict_temp[symbol_var] = time_series_df\n",
    "\n",
    "    \n",
    "    ## Exporting in batches is useful to catch errors, \n",
    "    ## and to also pick up where you left off if API fails while running overnight \n",
    "    pickle_path1= Path('../FilesExportCompleteFMP_big/data_complete_'+str_symbol1+'_'+str_symbol2+'.pkl')\n",
    "    save_obj(complete_dict_temp,pickle_path1)\n",
    "    pickle_path2= Path('../FilesExportIncompleteFMP_big/data_incomplete_'+str_symbol1+'_'+str_symbol2+'.pkl')\n",
    "    save_obj(incomplete_dict_temp,pickle_path2)\n",
    "    \n",
    "    ## Check before run, if incorrect, can waste a lot of API credits  \n",
    "    x += increment\n",
    "    y += increment\n",
    "    \n",
    "    ## Export in blocks of {increment} in-case there are errors while processing data. \n",
    "    ## Can pick up where the function left off by changing x and y vars to\n",
    "    ## avoid repeating API calls by doing this in order to not burn \n",
    "    ## IEX API tokens unnecessarily \n",
    "    \n",
    "    \n",
    "## If while loop finishes - export all data \n",
    "pkl_path_complete= Path('../Resources/all_FMP_data_complete.pkl')\n",
    "save_obj(complete_dict,pkl_path_complete)\n",
    "pkl_path_incomplete= Path('../Resources/all_FMP_data_incomplete.pkl')\n",
    "save_obj(incomplete_dict,pkl_path_incomplete)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
